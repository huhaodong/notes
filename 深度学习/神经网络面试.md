# 总结

## 1.常问问题

1） 什么是过拟合：过拟合是对训练样本过度描述，使得模型的繁华能力很弱，对于训练样本能获得很好的效果，但是在测试样本上的表现很差。

2） 如何解决过拟合问题：解决过拟合问题的主要方式是增加模型的普适性，在训练中的体现就是需要包含各种样本情况，使得模型训练的结果更加具有普适性。具体有：

- 增加样本的数量（或是增加噪声，样本本就不是无限多的，所以可以对原始样本集做旋转平移变化以及添加噪声数据。这使得模型可以适应各类环境下的目标，增加其泛化性能）；

- 通过dropout的操作每次随机忽略某些层的not，目的在于随机改变模型的结构，从而使得模型的计算结果在一定范围内波动，但是平均值是不变的；
  
- 选择合适的模型，模型越深效果越好，但是越容易发生过拟合的情况;

- 对权重进行正则化，将权重大小加到loss函数中$Loss = Loss+reg(w_i)$其中$reg(w_i)$分为$L_1$正则和$L_2$正则:

$$
    L_1=\frac{\lambda}{n}\sum_{i=0}^{n}|w_i|
$$
$$
    L_2=\frac{\lambda}{2n}\sum_{i=0}^{n}w_i^2
$$
通过对loss进行加权操作，因此在反向传播时全值的计算结果变小:
$$
未加权：w=w-\eta\frac{\delta c_0}{\delta w}
$$
$$
加权结果：w=(1-\frac{\eta \lambda}{n})w-\eta\frac{\delta c_0}{\delta w}
$$
$(1-\frac{\eta \lambda}{n})$小于1所以导致权重衰减，其目的时为了减少倒数过大的数量（缓解过拟合曲线的走势，使其平滑）

- 准确率不再增加时停止训练，以防止训练过度导致过拟合问题。
- 使用btach_normalization，

3）全连接层的作用：将提取的数据特征从一个特征空间映射到另一个特征空间，多数时候全连接层都被作为分类器。由于全连接成的参数很多，所以目前的趋势是使用卷积层代替全连接层。

4）激活函数的作用和原因：激活函数通常使用非线性函数，以增加模型的非线性，以此来提高拟合结果的泛化性。

5）卷积操作后输出的矩阵形状：
$$
ouptut_w=(\frac{image_w+2*pad+kernel}{stride})+1
$$

$$
ouptut_h=(\frac{image_h+2*pad+kernel}{stride})+1
$$

6）pooling 池化操作的作用：

- 对数据进行降维，减少参数的数量。
- 加强图像特征的不变性，使得模型的泛化能力提高。

7）为什么使用softmax而不是传统svm作为最后的分类器：对于数据的变动softmax比svm敏感，并且softmax是软分类svm是硬分类。

---

## 2.调优问题

1）数据集的注意事项：

- 需要对数据进行扩展，可以通过增加噪点、增白、减少像素、旋转平移变换、模糊等操作。
- 数据进入模型时先进行shuffle处理，每个epoch都进行shuffle效果会更好。
- 使用大数据训练模型前，先使用小数据集对模型进行训练，保证模型在小数据集上可以过拟合，再用大数据集进行训练。
- 小图片最好不要设置过大的步长，否则下采样过度导致信息丢失严重。
- 每个batch_size应该时128或256，如果内存不够可以适当减小。学习率应该是0.005或0.01左右，建议采用线性衰减。

2）卷积模型训练注意事项：

- 模型层数过高时，应该使用dropout降低模型过拟合的概率。
- 尽量使用relu或prelu激活函数，以避免传统激活函数带来的梯度消失和梯度爆炸等问题。

---

## 3.NIN模型

1）NIN（Network in network），使用1*1的卷积核链接再传统的卷积网络上形成mlponv（感知机卷积层）或cccp层。以使得网络再初级阶段就具有较高的特征提取能力，以此提升整体模型的模型表达能力。

2）mlp的作用是通过将提取的低级特征组合成复杂的组，以此增加卷积的特征有效性。

---

## 4.googlenet